import os
import tempfile
import logging
from typing import List, Optional, Dict, Any
import openai
import requests
import pickle
import json
import numpy as np
from datetime import datetime

logger = logging.getLogger(__name__)

class RAGSystem:
    """í–¥ìƒëœ RAG ì‹œìŠ¤í…œ - OpenAI API ì§ì ‘ ì‚¬ìš©"""
    
    def __init__(self, storage=None, chunk_size: int = 1200, chunk_overlap: int = 200):
        self.storage = storage
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.documents = []
        self.embeddings = []
        self.vector_store = {}
        self.embedding_model = "text-embedding-3-large"
        self.llm_model = "gpt-3.5-turbo"
        
        # ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ (API í‚¤ì™€ ë¬´ê´€í•˜ê²Œ ë¡œë“œ)
        self._load_vector_store()
        
        # OpenAI API í‚¤ ì„¤ì •
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        if not self.openai_api_key:
            logger.warning("âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì§ˆì˜ì‘ë‹µ ê¸°ëŠ¥ì€ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            openai.api_key = self.openai_api_key
            logger.info("âœ… OpenAI API í‚¤ ì„¤ì • ì™„ë£Œ")
            
        logger.info("âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _load_vector_store(self):
        """ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ"""
        try:
            if self.storage:
                # Cloud Storageì¸ ê²½ìš°
                if hasattr(self.storage, 'bucket'):
                    try:
                        vector_blob = self.storage.bucket.blob("vector_store/vector_store.pkl")
                        if vector_blob.exists():
                            vector_data = vector_blob.download_as_bytes()
                            data = pickle.loads(vector_data)
                            self.documents = data.get('documents', [])
                            self.embeddings = data.get('embeddings', [])
                            self.vector_store = data.get('vector_store', {})
                            logger.info(f"âœ… Cloud Storageì—ì„œ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì™„ë£Œ: {len(self.documents)}ê°œ ë¬¸ì„œ, {len(self.embeddings)}ê°œ ì„ë² ë”©")
                            
                            # ë²¡í„° ì €ì¥ì†Œê°€ ë¹„ì–´ìˆê±°ë‚˜ ìŠ¤í† ë¦¬ì§€ì— íŒŒì¼ì´ ë” ë§ì€ ê²½ìš° ìë™ ì„ë² ë”©
                            if self.storage:
                                files = self.storage.list_files()
                                logger.info(f"ğŸ“ ìŠ¤í† ë¦¬ì§€ì—ì„œ {len(files)}ê°œ íŒŒì¼ ë°œê²¬")
                                
                                # ë²¡í„° ì €ì¥ì†Œì˜ ë¬¸ì„œ ìˆ˜ì™€ ìŠ¤í† ë¦¬ì§€ì˜ íŒŒì¼ ìˆ˜ ë¹„êµ
                                if len(self.documents) == 0 or len(self.documents) < len(files):
                                    logger.info("ğŸ” ë²¡í„° ì €ì¥ì†Œê°€ ë¹„ì–´ìˆê±°ë‚˜ ë¶ˆì™„ì „í•©ë‹ˆë‹¤. ê¸°ì¡´ íŒŒì¼ë“¤ì„ í™•ì¸í•©ë‹ˆë‹¤...")
                                    try:
                                        for file_info in files:
                                            try:
                                                file_url = file_info.get('url')
                                                original_name = file_info.get('name')  # ì›ë³¸ íŒŒì¼ëª…
                                                
                                                if file_url and original_name:
                                                    logger.info(f"ğŸ“„ ìë™ ì„ë² ë”© ì‹œì‘: {original_name}")
                                                    success = self.add_document(file_url, original_name)
                                                    if success:
                                                        logger.info(f"âœ… ìë™ ì„ë² ë”© ì™„ë£Œ: {original_name}")
                                                    else:
                                                        logger.error(f"âŒ ìë™ ì„ë² ë”© ì‹¤íŒ¨: {original_name}")
                                            except Exception as e:
                                                logger.error(f"âŒ ìë™ ì„ë² ë”© ì¤‘ ì˜¤ë¥˜: {file_info.get('name', 'unknown')} - {e}")
                                    except Exception as e:
                                        logger.error(f"âŒ ê¸°ì¡´ íŒŒì¼ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
                        else:
                            logger.warning("âš ï¸ Cloud Storageì— ë²¡í„° ì €ì¥ì†Œ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.")
                    except Exception as e:
                        logger.error(f"âŒ Cloud Storage ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì‹¤íŒ¨: {e}")
                else:
                    # ë¡œì»¬ ìŠ¤í† ë¦¬ì§€ì¸ ê²½ìš°
                    vector_file = os.path.join(self.storage.local_dir, "vector_store.pkl")
                    logger.info(f"ğŸ” ë²¡í„° íŒŒì¼ ê²½ë¡œ: {vector_file}")
                    logger.info(f"ğŸ” ë²¡í„° íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(vector_file)}")
                    
                    if os.path.exists(vector_file):
                        with open(vector_file, 'rb') as f:
                            data = pickle.load(f)
                            self.documents = data.get('documents', [])
                            self.embeddings = data.get('embeddings', [])
                            self.vector_store = data.get('vector_store', {})
                        logger.info(f"âœ… ë¡œì»¬ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì™„ë£Œ: {len(self.documents)}ê°œ ë¬¸ì„œ, {len(self.embeddings)}ê°œ ì„ë² ë”©")
                    else:
                        logger.warning("âš ï¸ ë¡œì»¬ ë²¡í„° ì €ì¥ì†Œ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.")
            else:
                logger.warning("âš ï¸ ìŠ¤í† ë¦¬ì§€ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"âŒ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(f"âŒ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
    
    def _save_vector_store(self):
        """ë²¡í„° ì €ì¥ì†Œ ì €ì¥"""
        try:
            if self.storage:
                data = {
                    'documents': self.documents,
                    'embeddings': self.embeddings,
                    'vector_store': self.vector_store
                }
                
                # Cloud Storageì¸ ê²½ìš°
                if hasattr(self.storage, 'bucket'):
                    try:
                        vector_blob = self.storage.bucket.blob("vector_store/vector_store.pkl")
                        vector_data = pickle.dumps(data)
                        vector_blob.upload_from_string(vector_data, content_type='application/octet-stream')
                        logger.info("âœ… Cloud Storageì— ë²¡í„° ì €ì¥ì†Œ ì €ì¥ ì™„ë£Œ")
                    except Exception as e:
                        logger.error(f"âŒ Cloud Storage ë²¡í„° ì €ì¥ì†Œ ì €ì¥ ì‹¤íŒ¨: {e}")
                else:
                    # ë¡œì»¬ ìŠ¤í† ë¦¬ì§€ì¸ ê²½ìš°
                    vector_file = os.path.join(self.storage.local_dir, "vector_store.pkl")
                    with open(vector_file, 'wb') as f:
                        pickle.dump(data, f)
                    logger.info("âœ… ë¡œì»¬ ë²¡í„° ì €ì¥ì†Œ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ë²¡í„° ì €ì¥ì†Œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _get_embedding(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        try:
            response = openai.Embedding.create(
                model=self.embedding_model,
                input=text
            )
            return response['data'][0]['embedding']
        except Exception as e:
            logger.error(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return []
    
    def _split_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  (ì•ˆì „í•œ ë²„ì „)"""
        try:
            # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ë¶„í• 
            chunks = []
            start = 0
            
            while start < len(text):
                end = start + self.chunk_size
                chunk = text[start:end]
                chunks.append(chunk)
                start = end - self.chunk_overlap
                if start >= len(text):
                    break
                    
            return chunks
        except Exception as e:
            logger.error(f"âŒ í…ìŠ¤íŠ¸ ë¶„í•  ì‹¤íŒ¨: {e}")
            # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ë¶„í• 
            return [text[i:i+self.chunk_size] for i in range(0, len(text), self.chunk_size)]
    
    def add_document(self, file_url: str, filename: str) -> bool:
        """ë¬¸ì„œ ì¶”ê°€"""
        try:
            if not self.storage:
                logger.error("âŒ ìŠ¤í† ë¦¬ì§€ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return False
            
            # file_urlì—ì„œ ì‹¤ì œ ì €ì¥ëœ íŒŒì¼ëª… ì¶”ì¶œ
            if file_url.startswith('local://'):
                stored_filename = file_url.replace('local://', '')
            else:
                stored_filename = filename
            
            # íŒŒì¼ëª…ì—ì„œ ì‹¤ì œ íŒŒì¼ëª… ì¶”ì¶œ (ë©”íƒ€ë°ì´í„°ì—ì„œ ì›ë³¸ëª… ê°€ì ¸ì˜¤ê¸°)
            actual_filename = stored_filename
            try:
                # ë©”íƒ€ë°ì´í„°ì—ì„œ ì›ë³¸ íŒŒì¼ëª… ê°€ì ¸ì˜¤ê¸°
                if self.storage:
                    metadata = self.storage.get_metadata()
                    if stored_filename in metadata and 'original_name' in metadata[stored_filename]:
                        original_name = metadata[stored_filename]['original_name']
                        # í™•ì¥ì ì œê±°
                        if '.' in original_name:
                            actual_filename = original_name.rsplit('.', 1)[0]
                        else:
                            actual_filename = original_name
                        logger.info(f"ğŸ“„ ì›ë³¸ íŒŒì¼ëª… ì‚¬ìš©: {actual_filename}")
                    else:
                        # ë©”íƒ€ë°ì´í„°ê°€ ì—†ìœ¼ë©´ í™•ì¥ìë§Œ ì œê±°
                        if '.' in stored_filename:
                            actual_filename = stored_filename.rsplit('.', 1)[0]
                        logger.info(f"ğŸ“„ ì €ì¥ëœ íŒŒì¼ëª… ì‚¬ìš© (í™•ì¥ì ì œê±°): {actual_filename}")
            except Exception as e:
                logger.warning(f"âš ï¸ íŒŒì¼ëª… ì¶”ì¶œ ì‹¤íŒ¨, ì›ë³¸ ì‚¬ìš©: {e}")
                actual_filename = stored_filename
                if '.' in actual_filename:
                    actual_filename = actual_filename.rsplit('.', 1)[0]
            
            logger.info(f"ğŸ“„ ë¬¸ì„œ ì¶”ê°€ ì‹œì‘: {actual_filename} (ì €ì¥ëœ íŒŒì¼ëª…: {stored_filename})")
            
            # ë¬¸ì„œ ë¡œë“œ
            content = self._load_document(file_url, stored_filename)
            if not content:
                logger.error(f"âŒ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {stored_filename}")
                return False
            
            # í…ìŠ¤íŠ¸ ë¶„í• 
            chunks = self._split_text(content)
            logger.info(f"ğŸ“ í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")
            
            # ê° ì²­í¬ì— ëŒ€í•´ ì„ë² ë”© ìƒì„±
            for i, chunk in enumerate(chunks):
                embedding = self._get_embedding(chunk)
                if embedding:
                    self.documents.append({
                        'content': chunk,
                        'filename': actual_filename,
                        'stored_filename': stored_filename,
                        'chunk_id': i
                    })
                    self.embeddings.append(embedding)
                    self.vector_store[f"{actual_filename}_{i}"] = embedding
            
            # ë²¡í„° ì €ì¥ì†Œ ì €ì¥
            self._save_vector_store()
            
            # ìŠ¤í† ë¦¬ì§€ì— ì„ë² ë”© ìƒíƒœ í‘œì‹œ
            self.storage.mark_embedding_status(stored_filename, True)
            
            logger.info(f"âœ… ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ: {actual_filename} ({len(chunks)}ê°œ ì²­í¬)")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {filename} - {e}")
            return False
    
    def _load_document(self, file_url: str, filename: str) -> Optional[str]:
        """ë¬¸ì„œ ë¡œë“œ"""
        try:
            # íŒŒì¼ í™•ì¥ì í™•ì¸
            file_ext = filename.lower().split('.')[-1]
            
            logger.info(f"ğŸ“– ë¬¸ì„œ ë¡œë“œ ì‹œì‘: {filename} (í™•ì¥ì: {file_ext})")
            
            # ì„ì‹œ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œ
            with tempfile.NamedTemporaryFile(delete=False, suffix=f".{file_ext}") as temp_file:
                if file_url.startswith('local://'):
                    # ë¡œì»¬ íŒŒì¼ì—ì„œ ë‹¤ìš´ë¡œë“œ
                    if self.storage:
                        content = self.storage.download_file(filename)
                        if content:
                            temp_file.write(content)
                            logger.info(f"âœ… ë¡œì»¬ íŒŒì¼ì—ì„œ ë‹¤ìš´ë¡œë“œ: {len(content)} bytes")
                        else:
                            raise ValueError(f"ë¡œì»¬ íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}")
                elif file_url.startswith('gs://'):
                    # Google Cloud Storage URLì—ì„œ ë‹¤ìš´ë¡œë“œ
                    if self.storage and hasattr(self.storage, 'bucket'):
                        # Cloud Storage í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
                        blob_name = file_url.replace(f"gs://{self.storage.bucket_name}/", "")
                        blob = self.storage.bucket.blob(blob_name)
                        content = blob.download_as_bytes()
                        temp_file.write(content)
                        logger.info(f"âœ… Cloud Storageì—ì„œ ë‹¤ìš´ë¡œë“œ: {len(content)} bytes")
                    else:
                        raise ValueError("Cloud Storage í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                else:
                    # HTTP URLì—ì„œ ë‹¤ìš´ë¡œë“œ
                    response = requests.get(file_url)
                    response.raise_for_status()
                    temp_file.write(response.content)
                    logger.info(f"âœ… HTTP URLì—ì„œ ë‹¤ìš´ë¡œë“œ: {len(response.content)} bytes")
                
                temp_file_path = temp_file.name
            
            # íŒŒì¼ ë‚´ìš© ì½ê¸°
            if file_ext == 'pdf':
                try:
                    import PyPDF2
                    with open(temp_file_path, 'rb') as f:
                        pdf_reader = PyPDF2.PdfReader(f)
                        text = ""
                        for page in pdf_reader.pages:
                            text += page.extract_text() + "\n"
                except ImportError:
                    logger.error("âŒ PyPDF2ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                    return None
            elif file_ext in ['docx', 'doc']:
                try:
                    import docx2txt
                    text = docx2txt.process(temp_file_path)
                except ImportError:
                    logger.error("âŒ docx2txtê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                    return None
            elif file_ext == 'txt':
                with open(temp_file_path, 'r', encoding='utf-8') as f:
                    text = f.read()
            else:
                logger.error(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_ext}")
                return None
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.unlink(temp_file_path)
            
            logger.info(f"âœ… ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {filename} ({len(text)} ë¬¸ì)")
            return text
            
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {filename} - {e}")
            return None
    
    def query(self, question: str, chat_history: list = None) -> str:
        """ì§ˆì˜ì‘ë‹µ (ë§¥ë½ ì§€ì›)"""
        try:
            logger.info(f"ğŸ” ì§ˆì˜ ì‹œì‘: {question[:50]}...")
            logger.info(f"ğŸ” í˜„ì¬ ì„ë² ë”© ìˆ˜: {len(self.embeddings)}")
            logger.info(f"ğŸ” í˜„ì¬ ë¬¸ì„œ ìˆ˜: {len(self.documents)}")
            
            if not self.embeddings:
                logger.warning("âš ï¸ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
                return "ì•„ì§ ë¬¸ì„œê°€ ì¶”ê°€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
            
            if not self.openai_api_key:
                logger.warning("âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."
            
            # íŒŒì¼ëª… ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•œ íŠ¹ë³„ ì²˜ë¦¬
            filename_keywords = [
                'íŒŒì¼ëª…', 'ë¬¸ì„œëª…', 'ì €ì¥í•˜ê³  ìˆëŠ”', 'ì—…ë¡œë“œëœ', 'ë¬¸ì„œê°€ ë­', 
                'ì €ì¥ë˜ì–´ ìˆëŠ”', 'ë¬¸ì„œ ëª©ë¡', 'ëª©ë¡', 'ë¦¬ìŠ¤íŠ¸', 'ì–´ë–¤ ë¬¸ì„œ',
                'ë¬´ìŠ¨ ë¬¸ì„œ', 'ë¬¸ì„œë“¤', 'íŒŒì¼ë“¤', 'ì—…ë¡œë“œí•œ', 'ë“±ë¡ëœ'
            ]
            if any(keyword in question.lower() for keyword in filename_keywords):
                return self._handle_filename_question(question)
            
            # ì§ˆë¬¸ ì„ë² ë”© ìƒì„±
            question_embedding = self._get_embedding(question)
            if not question_embedding:
                return "ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            
            # ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸° (ìƒìœ„ 5ê°œë¡œ í™•ì¥)
            similarities = []
            for i, doc_embedding in enumerate(self.embeddings):
                score = self._cosine_similarity(question_embedding, doc_embedding)
                similarities.append((score, i))
            
            # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ 5ê°œ ì„ íƒ (ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ)
            similarities.sort(reverse=True)
            top_docs = similarities[:5]
            
            # ë””ë²„ê¹…ì„ ìœ„í•œ ë¡œê·¸ ì¶”ê°€
            logger.info(f"ğŸ” ê²€ìƒ‰ ê²°ê³¼: ìƒìœ„ 5ê°œ ìœ ì‚¬ë„ ì ìˆ˜ = {[f'{score:.3f}' for score, _ in top_docs[:5]]}")
            
            if not top_docs or top_docs[0][0] < 0.03:  # ìœ ì‚¬ë„ ì„ê³„ê°’ ë” ë‚®ì¶¤
                logger.warning(f"âš ï¸ ìœ ì‚¬ë„ê°€ ë„ˆë¬´ ë‚®ìŒ: ìµœê³  ì ìˆ˜ = {top_docs[0][0] if top_docs else 0}")
                return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # ìƒìœ„ ë¬¸ì„œë“¤ì˜ ë‚´ìš©ì„ ê²°í•© (ì—°ì†ëœ ì²­í¬ ì—°ê²°)
            context = ""
            processed_docs = set()  # ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€
            
            for score, idx in top_docs:
                if score > 0.1 and idx not in processed_docs:  # ìœ ì‚¬ë„ê°€ ë‚®ì€ ë¬¸ì„œëŠ” ì œì™¸
                    # íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì œê±°í•˜ê³  ë” ëª…í™•í•˜ê²Œ í‘œì‹œ
                    filename = self.documents[idx]['filename']
                    if '.' in filename:
                        display_name = filename.rsplit('.', 1)[0]  # í™•ì¥ì ì œê±°
                    else:
                        display_name = filename
                    
                    # ì—°ì†ëœ ì²­í¬ë“¤ì„ ì°¾ì•„ì„œ ì—°ê²°
                    connected_chunks = self._get_connected_chunks(idx, display_name)
                    context += f"\n\n=== {display_name} ===\n{connected_chunks}"
                    
                    # ì²˜ë¦¬ëœ ì²­í¬ë“¤ ë§ˆí‚¹
                    for chunk_idx in self._get_related_chunk_indices(idx):
                        processed_docs.add(chunk_idx)
            
            # ë§¥ë½ ì •ë³´ ì¶”ê°€ (ì´ì „ ëŒ€í™”)
            context_info = ""
            if chat_history:
                relevant_contexts = self._select_relevant_context(question, chat_history)
                if relevant_contexts:
                    context_info = "\n\nì´ì „ ëŒ€í™” ë§¥ë½:\n"
                    for conv in relevant_contexts:
                        context_info += f"Q: {conv['question']}\nA: {conv['answer']}\n\n"
            
            # OpenAIë¡œ ë‹µë³€ ìƒì„±
            prompt = f"""ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
ê° ë¬¸ì„œì˜ ì œëª©(íŒŒì¼ëª…)ì„ ì£¼ì˜ ê¹Šê²Œ ì‚´í´ë³´ê³ , í•´ë‹¹ ë¬¸ì„œì™€ ê´€ë ¨ëœ ë‚´ìš©ì„ ìš°ì„ ì ìœ¼ë¡œ ì°¸ê³ í•´ì£¼ì„¸ìš”.

**ì ˆëŒ€ ê¸ˆì§€ì‚¬í•­:**
- ì œê³µëœ ë¬¸ì„œì— ì—†ëŠ” ì¡°í•­, ë²•ë ¹, ê·œì •ì„ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”
- ì™¸ë¶€ ì§€ì‹ì´ë‚˜ ì¼ë°˜ì ì¸ ë²•ë¥  ì§€ì‹ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
- ì‹œí–‰ë ¹, ì‹œí–‰ê·œì¹™ ë“± ë¬¸ì„œì— ì—†ëŠ” ë²•ë ¹ì„ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”

ì¤‘ìš” ì§€ì¹¨: 
1. ì˜¤ì§ ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.
2. "ì´", "ê·¸", "ìœ„ì—ì„œ", "ì•ì„œ", "í•´ë‹¹" ë“±ì˜ ì°¸ì¡° í‘œí˜„ì´ ìˆë‹¤ë©´ í•´ë‹¹ ë‚´ìš©ì„ ì°¾ì•„ ì—°ê²°í•´ì£¼ì„¸ìš”.
3. ë‹¨ê³„ë³„ ì„¤ëª…ì´ë‚˜ ìˆœì„œê°€ ìˆëŠ” ë‚´ìš©ì€ ê·¸ íë¦„ì„ ìœ ì§€í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.
4. ì œê³µëœ ë¬¸ì„œì— ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì´ ì—†ë‹¤ë©´, ì¶”ì¸¡í•˜ì§€ ë§ê³  "ì œê³µëœ ë¬¸ì„œì—ëŠ” í•´ë‹¹ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…í™•íˆ ë‹µë³€í•´ì£¼ì„¸ìš”.
5. ì´ì „ ëŒ€í™”ì—ì„œ ì–¸ê¸‰ëœ ì¡°í•­ì´ë‚˜ ê·œì •ì— ëŒ€í•œ ì¶”ê°€ ì§ˆë¬¸ì¸ ê²½ìš°, ì´ì „ ë‹µë³€ì˜ ë§¥ë½ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.
6. "ê´€ë ¨ ì¡°í•­", "ê·¸ê²ƒ", "í•´ë‹¹ ë‚´ìš©" ë“± ë§¥ë½ ì—°ê²° ì§ˆë¬¸ì˜ ê²½ìš°, ì´ì „ ëŒ€í™”ì—ì„œ ì–¸ê¸‰ëœ ì£¼ì œì™€ ì—°ê²°í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì°¸ê³  ë¬¸ì„œë“¤:{context}{context_info}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
            
            response = openai.ChatCompletion.create(
                model=self.llm_model,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. **ì¤‘ìš”: ì˜¤ì§ ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.** ì™¸ë¶€ ì§€ì‹ì´ë‚˜ ì¼ë°˜ì ì¸ ë²•ë¥  ì§€ì‹ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. ì‚¬ìš©ìê°€ 'ì „ì²´ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ë³´ì—¬ë‹¬ë¼'ê³  ìš”ì²­í•˜ë©´, í•´ë‹¹ ì¡°í•­ì˜ ëª¨ë“  ë‚´ìš©ì„ ë¹ ì§ì—†ì´ ì™„ì „íˆ ì œê³µí•´ì£¼ì„¸ìš”. ê° ë¬¸ì„œì˜ ì œëª©(íŒŒì¼ëª…)ì„ ì£¼ì˜ ê¹Šê²Œ ì‚´í´ë³´ê³ , í•´ë‹¹ ë¬¸ì„œì™€ ê´€ë ¨ëœ ë‚´ìš©ì„ ìš°ì„ ì ìœ¼ë¡œ ì°¸ê³ í•´ì£¼ì„¸ìš”. ì‚¬ìš©ìê°€ 'ì €ì¥í•˜ê³  ìˆëŠ” ë¬¸ì„œê°€ ë­ì§€?', 'íŒŒì¼ëª…ì„ ì•Œë ¤ë‹¬ë¼' ë“±ì˜ ì§ˆë¬¸ì„ í•˜ë©´, ì°¸ê³  ë¬¸ì„œë“¤ì—ì„œ íŒŒì¼ëª…(=== íŒŒì¼ëª… === í˜•íƒœ)ì„ ì°¾ì•„ì„œ ì •í™•íˆ ì•Œë ¤ì£¼ì„¸ìš”. **ì ˆëŒ€ë¡œ ì œê³µëœ ë¬¸ì„œì— ì—†ëŠ” ì¡°í•­, ë²•ë ¹, ê·œì •ì„ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.** ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³ , ë¬¸ì„œ ë‚´ìš©ê³¼ ì´ì „ ëŒ€í™” ë§¥ë½ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. ë§Œì•½ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì´ ì œê³µëœ ë¬¸ì„œì— ì—†ë‹¤ë©´, 'ì œê³µëœ ë¬¸ì„œì—ëŠ” í•´ë‹¹ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤. ë” ìì„¸í•œ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ê´€ë ¨ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.'ë¼ê³  ë‹µë³€í•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2000,
                temperature=0.3
            )
            
            answer = response.choices[0].message.content.strip()
            logger.info(f"âœ… ì§ˆì˜ì‘ë‹µ ì™„ë£Œ: {question[:50]}...")
            return answer
            
        except Exception as e:
            logger.error(f"âŒ ì§ˆì˜ì‘ë‹µ ì‹¤íŒ¨: {e}")
            return f"ì§ˆì˜ì‘ë‹µ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def _get_related_chunk_indices(self, chunk_idx: int) -> List[int]:
        """íŠ¹ì • ì²­í¬ì™€ ê´€ë ¨ëœ ëª¨ë“  ì²­í¬ ì¸ë±ìŠ¤ ë°˜í™˜"""
        related_indices = [chunk_idx]
        current_doc = self.documents[chunk_idx]
        filename = current_doc['filename']
        
        # ê°™ì€ íŒŒì¼ì˜ ëª¨ë“  ì²­í¬ ì°¾ê¸°
        for i, doc in enumerate(self.documents):
            if doc['filename'] == filename:
                related_indices.append(i)
        
        return related_indices
    
    def _get_connected_chunks(self, chunk_idx: int, display_name: str) -> str:
        """ì—°ì†ëœ ì²­í¬ë“¤ì„ ì—°ê²°í•´ì„œ ë°˜í™˜ (ìŠ¤ë§ˆíŠ¸ ì„ íƒ)"""
        current_doc = self.documents[chunk_idx]
        filename = current_doc['filename']
        
        # ê°™ì€ íŒŒì¼ì˜ ëª¨ë“  ì²­í¬ë“¤ì„ chunk_id ìˆœìœ¼ë¡œ ì •ë ¬
        same_file_chunks = []
        for i, doc in enumerate(self.documents):
            if doc['filename'] == filename:
                same_file_chunks.append((doc['chunk_id'], i, doc['content']))
        
        # chunk_id ìˆœìœ¼ë¡œ ì •ë ¬
        same_file_chunks.sort(key=lambda x: x[0])
        
        # í˜„ì¬ ì²­í¬ ì£¼ë³€ì˜ ì²­í¬ë“¤ì„ ìš°ì„  ì„ íƒ
        current_chunk_id = current_doc['chunk_id']
        selected_chunks = []
        
        # í˜„ì¬ ì²­í¬ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì•ë’¤ ì²­í¬ë“¤ ì„ íƒ
        for chunk_id, idx, content in same_file_chunks:
            if abs(chunk_id - current_chunk_id) <= 2:  # í˜„ì¬ ì²­í¬ Â±2 ë²”ìœ„
                selected_chunks.append((chunk_id, idx, content))
        
        # í† í° ì œí•œì„ ê³ ë ¤í•˜ì—¬ ìµœëŒ€ 3ê°œ ì²­í¬ë§Œ ì‚¬ìš©
        if len(selected_chunks) > 3:
            selected_chunks = selected_chunks[:3]
        
        # ì—°ì†ëœ ì²­í¬ë“¤ì„ ì—°ê²°
        connected_content = ""
        for chunk_id, idx, content in selected_chunks:
            connected_content += content + "\n"
        
        logger.info(f"ğŸ“„ {display_name}: {len(selected_chunks)}ê°œ ì²­í¬ ì—°ê²°ë¨ (ì „ì²´ {len(same_file_chunks)}ê°œ ì¤‘)")
        return connected_content.strip()
    
    def _handle_filename_question(self, question: str) -> str:
        """íŒŒì¼ëª… ê´€ë ¨ ì§ˆë¬¸ ì²˜ë¦¬"""
        try:
            # ëª¨ë“  ê³ ìœ í•œ íŒŒì¼ëª… ìˆ˜ì§‘
            unique_filenames = set()
            for doc in self.documents:
                unique_filenames.add(doc['filename'])
            
            if not unique_filenames:
                return "í˜„ì¬ ì €ì¥ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
            
            # íŒŒì¼ëª… ëª©ë¡ ìƒì„±
            filename_list = list(unique_filenames)
            filename_list.sort()  # ì•ŒíŒŒë²³ ìˆœìœ¼ë¡œ ì •ë ¬
            
            if len(filename_list) == 1:
                return f"í˜„ì¬ ì €ì¥ëœ ë¬¸ì„œëŠ” '{filename_list[0]}'ì…ë‹ˆë‹¤."
            else:
                filename_text = "\n".join([f"- {filename}" for filename in filename_list])
                return f"í˜„ì¬ ì €ì¥ëœ ë¬¸ì„œëŠ” ì´ {len(filename_list)}ê°œì…ë‹ˆë‹¤:\n\n{filename_text}"
                
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ëª… ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return "íŒŒì¼ëª… ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def remove_document(self, filename: str) -> bool:
        """ë¬¸ì„œ ì œê±°"""
        try:
            # í•´ë‹¹ íŒŒì¼ì˜ ëª¨ë“  ì²­í¬ ì œê±°
            indices_to_remove = []
            for i, doc in enumerate(self.documents):
                if doc['filename'] == filename:
                    indices_to_remove.append(i)
            
            # ì—­ìˆœìœ¼ë¡œ ì œê±° (ì¸ë±ìŠ¤ ë³€í™” ë°©ì§€)
            for i in reversed(indices_to_remove):
                del self.documents[i]
                del self.embeddings[i]
            
            # ë²¡í„° ì €ì¥ì†Œ ì—…ë°ì´íŠ¸
            self.vector_store = {}
            for i, doc in enumerate(self.documents):
                self.vector_store[f"{doc['filename']}_{doc['chunk_id']}"] = self.embeddings[i]
            
            # ì €ì¥
            self._save_vector_store()
            
            logger.info(f"âœ… ë¬¸ì„œ ì œê±° ì™„ë£Œ: {filename}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ì œê±° ì‹¤íŒ¨: {filename} - {e}")
            return False
    
    def rebuild_index(self) -> bool:
        """ì¸ë±ìŠ¤ ì¬êµ¬ì„±"""
        try:
            # ê¸°ì¡´ ë¬¸ì„œë“¤ë¡œ ì¸ë±ìŠ¤ ì¬êµ¬ì„±
            old_documents = self.documents.copy()
            old_embeddings = self.embeddings.copy()
            
            self.documents = []
            self.embeddings = []
            self.vector_store = {}
            
            for doc in old_documents:
                embedding = self._get_embedding(doc['content'])
                if embedding:
                    self.documents.append(doc)
                    self.embeddings.append(embedding)
                    self.vector_store[f"{doc['filename']}_{doc['chunk_id']}"] = embedding
            
            # ì €ì¥
            self._save_vector_store()
            
            logger.info("âœ… ì¸ë±ìŠ¤ ì¬êµ¬ì„± ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì¸ë±ìŠ¤ ì¬êµ¬ì„± ì‹¤íŒ¨: {e}")
            return False
    
    def clear_index(self) -> bool:
        """ì¸ë±ìŠ¤ ì´ˆê¸°í™”"""
        try:
            self.documents = []
            self.embeddings = []
            self.vector_store = {}
            
            # ì €ì¥
            self._save_vector_store()
            
            # ìŠ¤í† ë¦¬ì§€ì˜ ëª¨ë“  íŒŒì¼ ì„ë² ë”© ìƒíƒœë¥¼ Falseë¡œ ë³€ê²½
            if self.storage:
                files = self.storage.list_files()
                for file_info in files:
                    self.storage.mark_embedding_status(file_info['filename'], False)
            
            logger.info("âœ… ì¸ë±ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì¸ë±ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def get_status(self) -> Dict[str, Any]:
        """RAG ì‹œìŠ¤í…œ ìƒíƒœ ë°˜í™˜"""
        return {
            'total_documents': len(self.documents),
            'total_embeddings': len(self.embeddings),
            'embedding_model': self.embedding_model,
            'llm_model': self.llm_model,
            'chunk_size': self.chunk_size,
            'chunk_overlap': self.chunk_overlap,
            'is_initialized': self.openai_api_key is not None
        }
    
    def get_vector_db_info(self) -> Dict[str, Any]:
        """ë²¡í„° DB ìƒì„¸ ì •ë³´ ë°˜í™˜"""
        if not self.embeddings:
            return {
                'total_vectors': 0,
                'dimensions': 0,
                'db_size_mb': 0,
                'index_type': 'empty'
            }
        
        # ì²« ë²ˆì§¸ ì„ë² ë”©ì˜ ì°¨ì› ìˆ˜ í™•ì¸
        dimensions = len(self.embeddings[0]) if self.embeddings else 0
        
        # ë²¡í„° ì €ì¥ì†Œ íŒŒì¼ í¬ê¸°
        db_size = 0
        if self.storage:
            vector_file = os.path.join(self.storage.local_dir, "vector_store.pkl")
            if os.path.exists(vector_file):
                db_size = os.path.getsize(vector_file)
        
        return {
            'total_vectors': len(self.embeddings),
            'dimensions': dimensions,
            'db_size_mb': round(db_size / (1024**2), 2),
            'index_type': 'pickle',
            'storage_path': self.storage.local_dir if self.storage else 'unknown'
        }
    
    def search_test(self, query: str) -> Dict[str, Any]:
        """ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        try:
            if not self.embeddings:
                return {
                    'query': query,
                    'results': [],
                    'message': 'ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤.'
                }
            
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self._get_embedding(query)
            if not query_embedding:
                return {
                    'query': query,
                    'results': [],
                    'message': 'ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.'
                }
            
            # ìœ ì‚¬ë„ ê³„ì‚°
            similarities = []
            for i, embedding in enumerate(self.embeddings):
                similarity = self._cosine_similarity(query_embedding, embedding)
                similarities.append({
                    'index': i,
                    'similarity': similarity,
                    'document': self.documents[i] if i < len(self.documents) else 'Unknown'
                })
            
            # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            similarities.sort(key=lambda x: x['similarity'], reverse=True)
            
            return {
                'query': query,
                'results': similarities[:5],  # ìƒìœ„ 5ê°œ ê²°ê³¼
                'total_results': len(similarities),
                'message': 'ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.'
            }
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return {
                'query': query,
                'results': [],
                'message': f'ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}'
            }
    
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            vec1 = np.array(vec1)
            vec2 = np.array(vec2)
            
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            return dot_product / (norm1 * norm2)
            
        except Exception as e:
            logger.error(f"ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _extract_keywords(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°œì„ )"""
        # í•œêµ­ì–´ ë¶ˆìš©ì–´ ì œê±°
        stopwords = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ë¡œ', 'ìœ¼ë¡œ', 'ì—ì„œ', 'ì—ê²Œ', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ', 'ë¶€í„°', 'ê¹Œì§€', 'í•œ', 'ë‘', 'ì„¸', 'ë„¤', 'ë‹¤ì„¯', 'ì—¬ì„¯', 'ì¼ê³±', 'ì—¬ëŸ', 'ì•„í™‰', 'ì—´']
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬)
        words = text.split()
        keywords = []
        
        for word in words:
            # ë¶ˆìš©ì–´ ì œê±° ë° ê¸¸ì´ ì²´í¬
            if word not in stopwords and len(word) > 1:
                keywords.append(word)
        
        # ì¡°í•­ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ê°€
        if any(keyword in text for keyword in ['ì¡°', 'í•­', 'í˜¸', 'ëª©']):
            keywords.extend(['ì¡°í•­', 'ë²•ì¡°ë¬¸', 'ê·œì •'])
        
        return keywords
    
    def _extract_article_info(self, answer_text: str) -> List[str]:
        """ë‹µë³€ì—ì„œ ì¡°í•­ ì •ë³´ ì¶”ì¶œ"""
        import re
        
        # ì¡°í•­ íŒ¨í„´ ê°ì§€
        patterns = [
            r'ì œ\d+ì¡°',
            r'ì œ\d+í•­',
            r'ì œ\d+í˜¸',
            r'ì œ\d+ëª©',
            r'\d+ì¡°',
            r'\d+í•­',
            r'\d+í˜¸',
            r'\d+ëª©'
        ]
        
        article_info = []
        for pattern in patterns:
            matches = re.findall(pattern, answer_text)
            article_info.extend(matches)
        
        return article_info
    
    def _select_relevant_context(self, current_question: str, chat_history: list, max_contexts: int = 3) -> list:
        """ê°œì„ ëœ ë§¥ë½ ì„ íƒ (ì¡°í•­ ì •ë³´ ì¸ì‹)"""
        try:
            if not chat_history:
                return []
            # ìµœê·¼ 50ê°œ ëŒ€í™”ë§Œ ê²€ìƒ‰ (ì„±ëŠ¥ ìµœì í™”)
            recent_history = chat_history[-50:] if len(chat_history) > 50 else chat_history
            
            if not recent_history:
                return []
            
            # í˜„ì¬ ì§ˆë¬¸ì˜ í‚¤ì›Œë“œ ì¶”ì¶œ
            current_keywords = set(self._extract_keywords(current_question))
            
            # ë§¥ë½ ì—°ê²° ì§ˆë¬¸ì¸ì§€ í™•ì¸ (ê´€ë ¨, ì¡°í•­, ë‚´ìš© ë“±)
            context_connection_keywords = ['ê´€ë ¨', 'ì¡°í•­', 'ë‚´ìš©', 'ê·¸ê²ƒ', 'ì´ê²ƒ', 'í•´ë‹¹', 'ìœ„ì—ì„œ', 'ì•ì„œ']
            is_context_question = any(keyword in current_question for keyword in context_connection_keywords)
            
            # í‚¤ì›Œë“œê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ë§¥ë½ ì—°ê²° ì§ˆë¬¸ì¸ ê²½ìš° ìµœê·¼ ëŒ€í™”ë¥¼ ìš°ì„  ì„ íƒ
            if not current_keywords or len(current_keywords) < 2 or is_context_question:
                logger.info(f"ğŸ” ë§¥ë½ ì—°ê²° ì§ˆë¬¸ ë˜ëŠ” í‚¤ì›Œë“œ ë¶€ì¡±, ìµœê·¼ ëŒ€í™” ìš°ì„  ì„ íƒ: {current_keywords}, ë§¥ë½ì§ˆë¬¸: {is_context_question}")
                return recent_history[-1:] if recent_history else []
            
            # ì¡°í•­ ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ í™•ì¸ (ë” í¬ê´„ì ìœ¼ë¡œ)
            is_article_question = any(keyword in current_question for keyword in ['ì¡°', 'í•­', 'í˜¸', 'ëª©', 'ëª‡', 'ì¡°í•­', 'ê·œì •', 'ë‚´ìš©'])
            
            scored_contexts = []
            
            # ê° ëŒ€í™”ì™€ í‚¤ì›Œë“œ ìœ ì‚¬ë„ ê³„ì‚°
            for conv in recent_history:
                conv_text = conv['question'] + ' ' + conv['answer']
                conv_keywords = set(self._extract_keywords(conv_text))
                
                # ê¸°ë³¸ í‚¤ì›Œë“œ ìœ ì‚¬ë„
                overlap = len(current_keywords & conv_keywords)
                
                # ì¡°í•­ ê´€ë ¨ ì§ˆë¬¸ì¸ ê²½ìš° ì´ì „ ë‹µë³€ì—ì„œ ì¡°í•­ ì •ë³´ ì¶”ì¶œ
                if is_article_question:
                    article_info = self._extract_article_info(conv['answer'])
                    if article_info:
                        # ì¡°í•­ ì •ë³´ê°€ ìˆìœ¼ë©´ ë†’ì€ ì ìˆ˜ ë¶€ì—¬
                        overlap += len(article_info) * 2
                
                if overlap > 0:
                    # ìœ ì‚¬ë„ ì ìˆ˜ = ê²¹ì¹˜ëŠ” í‚¤ì›Œë“œ ìˆ˜ / í˜„ì¬ ì§ˆë¬¸ í‚¤ì›Œë“œ ìˆ˜
                    similarity_score = overlap / len(current_keywords) if current_keywords else 0
                    scored_contexts.append((similarity_score, conv))
            
            # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ Nê°œ ì„ íƒ
            scored_contexts.sort(reverse=True)
            selected_contexts = [conv for _, conv in scored_contexts[:max_contexts]]
            
            logger.info(f"âœ… ë§¥ë½ ì„ íƒ ì™„ë£Œ: {len(selected_contexts)}ê°œ ëŒ€í™” ì„ íƒ (ì¡°í•­ ì§ˆë¬¸: {is_article_question}, ì´ íˆìŠ¤í† ë¦¬: {len(chat_history)}ê°œ)")
            return selected_contexts
            
        except Exception as e:
            logger.error(f"âŒ ë§¥ë½ ì„ íƒ ì‹¤íŒ¨: {e}")
            return []
    
    def backup_vectors(self, backup_path: str) -> bool:
        """ë²¡í„° ì €ì¥ì†Œ ë°±ì—…"""
        try:
            backup_data = {
                'documents': self.documents,
                'embeddings': self.embeddings,
                'vector_store': self.vector_store,
                'backup_timestamp': datetime.now().isoformat()
            }
            
            with open(backup_path, 'wb') as f:
                pickle.dump(backup_data, f)
            
            logger.info(f"âœ… ë²¡í„° ì €ì¥ì†Œ ë°±ì—… ì™„ë£Œ: {backup_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ë²¡í„° ì €ì¥ì†Œ ë°±ì—… ì‹¤íŒ¨: {e}")
            return False
    
    def restore_vectors(self, backup_path: str) -> bool:
        """ë²¡í„° ì €ì¥ì†Œ ë³µì›"""
        try:
            with open(backup_path, 'rb') as f:
                backup_data = pickle.load(f)
            
            self.documents = backup_data.get('documents', [])
            self.embeddings = backup_data.get('embeddings', [])
            self.vector_store = backup_data.get('vector_store', {})
            
            # ë²¡í„° ì €ì¥ì†Œ ì €ì¥
            self._save_vector_store()
            
            logger.info(f"âœ… ë²¡í„° ì €ì¥ì†Œ ë³µì› ì™„ë£Œ: {backup_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ë²¡í„° ì €ì¥ì†Œ ë³µì› ì‹¤íŒ¨: {e}")
            return False
    
    def update_settings(self, settings: Dict[str, Any]) -> bool:
        """ì‹œìŠ¤í…œ ì„¤ì • ì—…ë°ì´íŠ¸"""
        try:
            if 'chunk_size' in settings:
                self.chunk_size = settings['chunk_size']
            if 'chunk_overlap' in settings:
                self.chunk_overlap = settings['chunk_overlap']
            if 'embedding_model' in settings:
                self.embedding_model = settings['embedding_model']
            if 'llm_model' in settings:
                self.llm_model = settings['llm_model']
            
            logger.info("âœ… RAG ì‹œìŠ¤í…œ ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì„¤ì • ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            return False
    
    def get_settings(self) -> Dict[str, Any]:
        """í˜„ì¬ ì‹œìŠ¤í…œ ì„¤ì • ë°˜í™˜"""
        return {
            'chunk_size': self.chunk_size,
            'chunk_overlap': self.chunk_overlap,
            'embedding_model': self.embedding_model,
            'llm_model': self.llm_model,
            'openai_api_key_configured': self.openai_api_key is not None
        }
